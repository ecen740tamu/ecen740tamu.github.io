---
layout: default
title: Summary 1
permalink: /summary1/
mathjax: True
---

## Machine Learning Concepts to Remember

1. Classification and Regression Problems
2. The Bayesian Approach
3. The Frequentist Approach
4. The Learning Problem
5. Probably Approximately Correct (PAC) Learning
6. Empirical Risk Minimization
7. Measure Concentration
8. Vapnick-Chervonenkis Dimension
9. Characterization of UWLLN
10. PAC Learnablity
11. VC dimensions of and Halfspaces and Neural Networks
12. Computation Learning
13. Training Error, Test Error
14. Regularization
15. Representation Capability of Neural Networks
16. Training Neural Networks
17. Modifications in Training NNs
18. Convex Sets and Convex Functions
19. Stochastic Gradient Descent
20. CNNs, Autoencoders and GANs
21. Support Vector Machines
22. Boosting
23. Gaussian Process based Learning
24. Decision Trees and Random Forests
25. Clustering
26. Recurrent Neural Networks
27. LSTM Networks
28. Transformers
29. BERT
30. GPT


### Decision Trees and Random Forests

1. Decision Trees suffer with stability with a minor change in data, therefore must be replaced with random forest of
decision trees. 
2. Deep trees also decrease the accuracy, since they make increase over fitting, hence less bias more variances
3. Random forests is an ensemble of decision of trees, a way to average the multiple deep decision trees.
4. Random forests are constructed as ensemble of decision trees at training time.
5. Booststrap aggregating or bagging, to tree learners: Say training set has $X=x_1, x_2, ..x_n$ with, train B trees using samples from training set with replacements independently, in the testing phase use the outputs from the B trees
and use average or weighted majority response. This is also called boostrapp (idea of sampling data with replacement) aggregating since each trained tree is independent


### ML Metrics

1. Accuracy
2. Sensitivity
3. Specificity
4. Precision
5. Miss rate
6. False discovery rate
7. False omission rate


### Classification and Regression Problems

X = Feature Vector $x \in X$


### K means clustering: 
Unsupervised algorithm,

psuedo code: Generate some randmo

### KNN nearest neighbors: 
Supervised algorithm

Psuedo code: 


## Classification and Regression Problem
1. X is a feature vector $x \in X$
2. y is label

Goal: Given X, predict y

### Binary classification y = {0,1}
### Multinary Classification y = {0,1,2..4}

Regression $y \in R$

### Bayesian Approach
Suppose we know the joint probability distribution